# Simple Retrieval Agent Configuration
# This configuration is for a simple agent system focused on retrieval functionality

system:
  name: "Simple Retrieval Agent"
  version: "2.0.0"
  
  # Server configuration for agent communication
  server:
    host: "127.0.0.1"
    port: 8080
    timeout: 30
  
  # Logging configuration for agents
  logging:
    level: "INFO"    # DEBUG, INFO, WARNING, ERROR
    file: "retrieval_agent.log"
    console: true

# Define the agents in your system
agents:
  # Retrieval Agent - Handles document retrieval and knowledge search
  - name: "retrieval_agent"
    id: "retrieval-main"
    type: "specialist"
    role: "RETRIEVAL_SPECIALIST"
    specializations:
      - "DOCUMENT_RETRIEVAL"
      - "KNOWLEDGE_SEARCH"
      - "INFORMATION_EXTRACTION"
    capabilities:
      - "document_search"
      - "vector_similarity"
      - "knowledge_retrieval"
      - "embedding_generation"
    functions:
      - "server_document_retrieval"
      - "knowledge_retrieval"
      - "get_embedding"
    config:
      priority: 1
      auto_start: true
      max_concurrent_tasks: 5
      retrieval_threshold: 0.7

# Define custom functions available to agents
functions:
  # Document Retrieval Functions
  - name: "server_document_retrieval"
    type: "builtin"
    description: "Retrieve documents from server-based vector database using similarity search"
    parameters:
      - name: "query"
        type: "string"
        required: true
        description: "Search query to find relevant documents"
      - name: "max_results"
        type: "integer"
        required: false
        default: 10
        description: "Maximum number of documents to retrieve"
      - name: "score_threshold"
        type: "number"
        required: false
        default: 0.7
        description: "Minimum similarity score threshold"
      - name: "collection_name"
        type: "string"
        required: false
        default: "documents"
        description: "Name of the document collection to search"

  - name: "knowledge_retrieval"
    type: "builtin"
    description: "Comprehensive knowledge retrieval combining document search and web search"
    parameters:
      - name: "query"
        type: "string"
        required: true
        description: "Knowledge query to search for"
      - name: "max_results"
        type: "integer"
        required: false
        default: 15
        description: "Maximum number of results to return"
      - name: "local_only"
        type: "boolean"
        required: false
        default: false
        description: "Only search local documents, skip web search"
      - name: "web_only"
        type: "boolean"
        required: false
        default: false
        description: "Only search web, skip local documents"

  - name: "get_embedding"
    type: "builtin"
    description: "Generate text embeddings using server-side embedding models"
    parameters:
      - name: "text"
        type: "string"
        required: true
        description: "Text to generate embedding for"
      - name: "model_id"
        type: "string"
        required: false
        default: "qwen3-embedding-0.6b"
        description: "Embedding model to use"

# System-wide configuration
configuration:
  # Memory management
  memory:
    max_entries_per_agent: 500
    cleanup_interval: 300  # seconds
    persistence_enabled: true
    
  # Communication
  messaging:
    timeout: 30
    retry_attempts: 3
    broadcast_enabled: false
    
  # Performance
  performance:
    max_parallel_agents: 3
    task_queue_size: 50
    resource_monitoring: true
    
  # Security
  security:
    agent_isolation: true
    function_sandboxing: true
    resource_limits: true

# Inference engines configuration (Agent System specific)
inference_engines:
  - name: "llama-cpu"
    type: "llama"
    model_path: "D:\\Works\\Genta\\codes\\kolosal-agent\\models\\qwen2.5-0.5b-instruct-q4_k_m.gguf"
    description: "CPU-based inference engine for LLaMA models"
    auto_load: true
    context_size: 2048
    batch_size: 512
    gpu_layers: 0  # CPU only

# Default inference engine for agents
default_inference_engine: "llama-cpu"
